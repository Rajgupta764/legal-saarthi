"""
Legal Document Scanner Service
- OCR text extraction using OCR.space API (free) or pytesseract (offline)
- Text summarization and simplification using Google Gemini API
- Outputs simple Hindi explanations for rural users
"""

import os
import re
import base64
import requests
from datetime import datetime, timedelta
from io import BytesIO

# Try to import pytesseract for offline OCR (optional)
try:
    import pytesseract
    from PIL import Image
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False

# Try to import Google Generative AI
try:
    import google.genai as genai
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False


class DocumentService:
    """Service for analyzing legal documents with real OCR and AI"""
    
    def __init__(self):
        # OCR.space API key (free tier: 25,000 requests/month)
        # Get free key at: https://ocr.space/ocrapi
        self.ocr_api_key = os.environ.get('OCR_SPACE_API_KEY', 'K85557640888957')
        
        # Google Gemini API key (free tier available)
        # Get free key at: https://makersuite.google.com/app/apikey
        self.gemini_api_key = os.environ.get('GEMINI_API_KEY', '')
        
        # Initialize Gemini if available
        if GENAI_AVAILABLE and self.gemini_api_key:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel('gemini-pro')
        else:
            self.gemini_model = None
        
        # Document type patterns for classification
        self.doc_patterns = {
            'legal_notice': {
                'keywords': ['notice', '‡§®‡•ã‡§ü‡§ø‡§∏', 'hereby', 'demanded', 'legal', 'advocate', '‡§µ‡§ï‡•Ä‡§≤', '‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä', '‡§ö‡•á‡§§‡§æ‡§µ‡§®‡•Ä'],
                'name': '‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§®‡•ã‡§ü‡§ø‡§∏ (Legal Notice)',
                'urgency': 'high'
            },
            'fir': {
                'keywords': ['fir', 'first information report', '‡§™‡•ç‡§∞‡§•‡§Æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü', 'police', '‡§•‡§æ‡§®‡§æ', '‡§ß‡§æ‡§∞‡§æ', 'ipc', 'bns', 'crpc', 'bnss'],
                'name': '‡§™‡•ç‡§∞‡§•‡§Æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü (FIR)',
                'urgency': 'high'
            },
            'court_order': {
                'keywords': ['court', '‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø', 'order', '‡§Ü‡§¶‡•á‡§∂', 'judge', 'petition', 'case no', '‡§ï‡•á‡§∏ ‡§®‡§Ç‡§¨‡§∞', '‡§ú‡§ú', '‡§Æ‡§æ‡§®‡§®‡•Ä‡§Ø'],
                'name': '‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø ‡§Ü‡§¶‡•á‡§∂ (Court Order)',
                'urgency': 'high'
            },
            'land_record': {
                'keywords': ['‡§ñ‡§§‡•å‡§®‡•Ä', '‡§ñ‡§∏‡§∞‡§æ', '‡§≠‡•Ç‡§Æ‡§ø', 'land', 'plot', 'registry', 'deed', '‡§™‡§ü‡•ç‡§ü‡§æ', '‡§∞‡§ú‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•Ä', '‡§ú‡§Æ‡•Ä‡§®', '‡§∞‡§æ‡§ú‡§∏‡•ç‡§µ', '‡§§‡§π‡§∏‡•Ä‡§≤'],
                'name': '‡§≠‡•Ç‡§Æ‡§ø ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º (Land Record)',
                'urgency': 'medium'
            },
            'government_letter': {
                'keywords': ['government', '‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä', 'department', '‡§µ‡§ø‡§≠‡§æ‡§ó', 'office', '‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø', 'memo', 'circular', '‡§≠‡§æ‡§∞‡§§ ‡§∏‡§∞‡§ï‡§æ‡§∞', '‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§∏‡§∞‡§ï‡§æ‡§∞'],
                'name': '‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§™‡§§‡•ç‡§∞ (Government Letter)',
                'urgency': 'medium'
            },
            'agreement': {
                'keywords': ['agreement', '‡§∏‡§Æ‡§ù‡•å‡§§‡§æ', 'contract', '‡§Ö‡§®‡•Å‡§¨‡§Ç‡§ß', 'party', 'witness', '‡§ó‡§µ‡§æ‡§π', '‡§ï‡§∞‡§æ‡§∞', '‡§π‡§∏‡•ç‡§§‡§æ‡§ï‡•ç‡§∑‡§∞'],
                'name': '‡§∏‡§Æ‡§ù‡•å‡§§‡§æ ‡§™‡§§‡•ç‡§∞ (Agreement)',
                'urgency': 'medium'
            },
            'resume': {
                'keywords': ['resume', 'cv', 'curriculum', 'objective', 'qualifications', 'skills', 'experience', 'education', 
                            '‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á', '‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ', '‡§Ö‡§®‡•Å‡§≠‡§µ', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ'],
                'name': '‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á/CV (Resume)',
                'urgency': 'low'
            }
        }

    def analyze_document(self, file):
        """
        Main method to analyze uploaded document
        1. Extract text using OCR
        2. Classify document type
        3. Simplify text to Hindi
        4. Extract key points and dates
        """
        try:
            # Step 1: Extract text using OCR
            ocr_result = self._extract_text_ocr(file)
            
            if not ocr_result['success']:
                return {
                    'success': False,
                    'error': ocr_result.get('error', 'OCR failed'),
                    'message': '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§®‡§ø‡§ï‡§æ‡§≤ ‡§™‡§æ‡§è‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§∏‡§æ‡§´‡§º ‡§´‡•ã‡§ü‡•ã ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§'
                }
            
            extracted_text = ocr_result['text']
            
            if len(extracted_text.strip()) < 20:
                return {
                    'success': False,
                    'error': 'Insufficient text extracted',
                    'message': '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•á ‡§™‡§∞‡•ç‡§Ø‡§æ‡§™‡•ç‡§§ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§ö‡•ç‡§õ‡•Ä ‡§ï‡•ç‡§µ‡§æ‡§≤‡§ø‡§ü‡•Ä ‡§ï‡•Ä ‡§´‡•ã‡§ü‡•ã ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç‡•§'
                }
            
            # Step 2: Classify document type
            doc_type = self._classify_document(extracted_text)
            
            # Step 3: Simplify text (AI or rule-based)
            simplified = self._simplify_text(extracted_text, doc_type)
            
            # Step 4: Extract key information
            key_points = self._extract_key_points(extracted_text, doc_type)
            dates = self._extract_dates(extracted_text)
            
            # Step 5: Generate recommended actions
            actions = self._get_recommended_actions(doc_type)
            
            return {
                'success': True,
                'data': {
                    'documentType': doc_type,
                    'documentTypeName': self.doc_patterns.get(doc_type, {}).get('name', '‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º'),
                    'urgencyLevel': self.doc_patterns.get(doc_type, {}).get('urgency', 'normal'),
                    'extractedText': extracted_text[:2000] + ('...' if len(extracted_text) > 2000 else ''),
                    'simplifiedText': simplified,
                    'keyPoints': key_points,
                    'importantDates': dates,
                    'recommendedActions': actions,
                    'ocrMethod': ocr_result.get('method', 'Unknown'),
                    'wordCount': len(extracted_text.split()),
                    'processedAt': datetime.now().strftime('%d/%m/%Y %H:%M')
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'message': '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§π‡•Å‡§à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Å‡§®‡§É ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§'
            }

    def _extract_text_ocr(self, file):
        """
        Extract text from document using OCR
        Primary: OCR.space API (free, supports Hindi)
        Fallback: pytesseract (offline)
        """
        filename = file.filename.lower()
        is_pdf = filename.endswith('.pdf')
        
        print(f"[OCR DEBUG] Processing file: {filename}, is_pdf: {is_pdf}")
        
        # Read file content
        file_content = file.read()
        file.seek(0)  # Reset file pointer
        
        print(f"[OCR DEBUG] File content size: {len(file_content)} bytes")
        
        if len(file_content) == 0:
            return {
                'success': False,
                'error': 'File is empty'
            }
        
        # Try OCR.space API first (better Hindi support)
        ocr_error = None
        try:
            print(f"[OCR DEBUG] Calling OCR.space API...")
            result = self._ocr_space_api(file_content, is_pdf, filename)
            print(f"[OCR DEBUG] OCR.space result: success={result.get('success')}, error={result.get('error', 'none')}")
            if result['success']:
                return result
            ocr_error = result.get('error', 'Unknown OCR error')
        except Exception as e:
            print(f"[OCR DEBUG] OCR.space API exception: {e}")
            import traceback
            traceback.print_exc()
            ocr_error = str(e)
        
        # Fallback to pytesseract if available
        if PYTESSERACT_AVAILABLE and not is_pdf:
            try:
                print(f"[OCR DEBUG] Trying pytesseract fallback...")
                result = self._pytesseract_ocr(file_content)
                if result['success']:
                    return result
            except Exception as e:
                print(f"[OCR DEBUG] Pytesseract failed: {e}")
        
        return {
            'success': False,
            'error': ocr_error or 'All OCR methods failed. Please try with a clearer image.'
        }

    def _ocr_space_api(self, file_content, is_pdf=False, filename='image.png'):
        """
        OCR using OCR.space API (Free: 25,000 requests/month)
        Supports: Hindi, English, and many other languages
        Uses multipart form upload for better reliability
        """
        import io
        url = 'https://api.ocr.space/parse/image'
        
        # Determine file type and extension
        filename_lower = filename.lower()
        if is_pdf or filename_lower.endswith('.pdf'):
            mime = 'application/pdf'
            filetype = 'PDF'
        elif filename_lower.endswith('.png'):
            mime = 'image/png'
            filetype = 'PNG'
        elif filename_lower.endswith('.gif'):
            mime = 'image/gif'
            filetype = 'GIF'
        elif filename_lower.endswith('.bmp'):
            mime = 'image/bmp'
            filetype = 'BMP'
        else:
            # Default to JPEG for .jpg, .jpeg, or unknown
            mime = 'image/jpeg'
            filetype = 'JPG'
        
        # Use multipart form upload (more reliable than base64)
        files = {
            'file': (filename, io.BytesIO(file_content), mime)
        }
        
        payload = {
            'apikey': self.ocr_api_key,
            'filetype': filetype,
            'language': 'eng',  # English (works well for Hindi too with Engine 1)
            'isOverlayRequired': 'false',
            'detectOrientation': 'true',
            'scale': 'true',
            'OCREngine': '1',  # Engine 1 supports more languages
        }
        
        print(f"[OCR API DEBUG] Sending multipart request: filetype={filetype}, mime={mime}, file_size={len(file_content)} bytes")
        
        response = requests.post(url, files=files, data=payload, timeout=60)
        print(f"[OCR API DEBUG] Response status: {response.status_code}")
        data = response.json()
        print(f"[OCR API DEBUG] Response data: OCRExitCode={data.get('OCRExitCode')}, IsErrored={data.get('IsErroredOnProcessing')}, ErrorMsg={data.get('ErrorMessage', 'none')}")
        
        if data.get('IsErroredOnProcessing', False):
            error_messages = data.get('ErrorMessage', ['Unknown error'])
            if isinstance(error_messages, list):
                error_msg = error_messages[0] if error_messages else 'Unknown error'
            else:
                error_msg = str(error_messages)
            return {
                'success': False,
                'error': error_msg
            }
        
        # Extract text from all pages
        parsed_results = data.get('ParsedResults', [])
        if not parsed_results:
            return {
                'success': False,
                'error': 'No text found in document'
            }
        
        full_text = '\n'.join([r.get('ParsedText', '') for r in parsed_results])
        
        return {
            'success': True,
            'text': full_text.strip(),
            'method': 'OCR.space API (Cloud)'
        }

    def _pytesseract_ocr(self, file_content):
        """
        Offline OCR using pytesseract
        Requires: Tesseract-OCR installed on system
        """
        if not PYTESSERACT_AVAILABLE:
            return {'success': False, 'error': 'Pytesseract not available'}
        
        try:
            # Open image
            image = Image.open(BytesIO(file_content))
            
            # OCR with Hindi + English
            text = pytesseract.image_to_string(image, lang='hin+eng')
            
            return {
                'success': True,
                'text': text.strip(),
                'method': 'Pytesseract (Offline)'
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def _classify_document(self, text):
        """Classify document type based on content keywords with minimum threshold"""
        text_lower = text.lower()
        
        # First, check if it's a resume
        resume_keywords = ['skills', 'experience', 'education', 'objective', 'summary', 'qualification', 
                          '‡§ï‡•å‡§∂‡§≤', '‡§Ö‡§®‡•Å‡§≠‡§µ', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', '‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ', 'b.tech', 'b.e', 'mba', 'b.sc']
        resume_score = sum(1 for keyword in resume_keywords if keyword in text_lower)
        
        if resume_score >= 3:  # At least 3 resume-related keywords
            return 'resume'
        
        scores = {}
        for doc_type, config in self.doc_patterns.items():
            score = sum(1 for keyword in config['keywords'] if keyword.lower() in text_lower)
            scores[doc_type] = score
        
        # Get highest scoring type with minimum threshold
        max_score = max(scores.values()) if scores else 0
        
        # Require at least 2 matching keywords for classification
        if max_score >= 2:
            return max(scores, key=scores.get)
        
        return 'general'

    def _simplify_text(self, text, doc_type):
        """
        Simplify legal text to simple Hindi
        Primary: Google Gemini API
        Fallback: Rule-based simplification
        """
        # Try AI simplification first
        if self.gemini_model:
            try:
                simplified = self._ai_simplify(text, doc_type)
                if simplified:
                    return simplified
            except Exception as e:
                print(f"Gemini API failed: {e}")
        
        # Fallback to rule-based simplification
        return self._rule_based_simplify(text, doc_type)

    def _ai_simplify(self, text, doc_type):
        """Use Google Gemini to simplify text"""
        if not self.gemini_model:
            return None
        
        prompt = f"""You are a legal assistant helping rural Indian users understand legal documents.
        
Task: Simplify the following legal document text into very simple Hindi that a village person can understand.

Document Type: {doc_type}

Original Text:
{text[:3000]}

Instructions:
1. Explain what this document is about in 2-3 simple lines
2. List what the person needs to do (if anything)
3. Mention any important deadlines
4. Use simple Hindi words, avoid English legal terms
5. Be direct and clear
6. If there are any warnings or urgent matters, highlight them

Format your response as:
üìã ‡§á‡§∏ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨:
[explanation]

‚úÖ ‡§Ü‡§™‡§ï‡•ã ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à:
[actions if any]

‚ö†Ô∏è ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç:
[important warnings]
"""
        
        response = self.gemini_model.generate_content(prompt)
        return response.text

    def _rule_based_simplify(self, text, doc_type):
        """
        Rule-based text simplification that actually uses the extracted content.
        Creates a summary based on REAL data found in the document.
        """
        import re
        
        # Extract actual information from the text
        extracted_info = self._extract_all_info(text)
        
        # Build a summary based on actual content
        summary_parts = []
        
        # Document type header with specific info
        doc_type_headers = {
            'legal_notice': 'üìã ‡§Ø‡§π ‡§è‡§ï ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§π‡•à',
            'fir': 'üìã ‡§Ø‡§π ‡§è‡§ï FIR (‡§™‡•ç‡§∞‡§•‡§Æ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§∞‡§ø‡§™‡•ã‡§∞‡•ç‡§ü) ‡§π‡•à',
            'court_order': 'üìã ‡§Ø‡§π ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡§æ ‡§Ü‡§¶‡•á‡§∂ ‡§π‡•à',
            'land_record': 'üìã ‡§Ø‡§π ‡§≠‡•Ç‡§Æ‡§ø/‡§ú‡§Æ‡•Ä‡§® ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§π‡•à',
            'government_letter': 'üìã ‡§Ø‡§π ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§™‡§§‡•ç‡§∞ ‡§π‡•à',
            'agreement': 'üìã ‡§Ø‡§π ‡§∏‡§Æ‡§ù‡•å‡§§‡§æ/‡§Ö‡§®‡•Å‡§¨‡§Ç‡§ß ‡§™‡§§‡•ç‡§∞ ‡§π‡•à',
            'resume': 'üéì ‡§Ø‡§π ‡§è‡§ï ‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á/CV ‡§π‡•à',
            'general': 'üìã ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä'
        }
        
        summary_parts.append(doc_type_headers.get(doc_type, 'üìã ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä'))
        summary_parts.append("")
        
        # Add actual extracted information
        summary_parts.append("üìù ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§à ‡§ó‡§à ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä:")
        
        info_found = False
        
        # Names found
        if extracted_info.get('names'):
            names_list = list(set(extracted_info['names']))[:5]  # Top 5 unique names
            summary_parts.append(f"‚Ä¢ ‡§®‡§æ‡§Æ: {', '.join(names_list)}")
            info_found = True
        
        # Dates found
        if extracted_info.get('dates'):
            dates_list = list(set(extracted_info['dates']))[:3]  # Top 3 dates
            summary_parts.append(f"‚Ä¢ ‡§§‡§æ‡§∞‡•Ä‡§ñ: {', '.join(dates_list)}")
            info_found = True
        
        # Amounts found
        if extracted_info.get('amounts'):
            amounts_list = list(set(extracted_info['amounts']))[:3]
            summary_parts.append(f"‚Ä¢ ‡§∞‡§æ‡§∂‡§ø: {', '.join(['‚Çπ' + a for a in amounts_list])}")
            info_found = True
        
        # Case/Reference numbers
        if extracted_info.get('case_numbers'):
            summary_parts.append(f"‚Ä¢ ‡§∏‡§Ç‡§¶‡§∞‡•ç‡§≠/‡§ï‡•á‡§∏ ‡§®‡§Ç‡§¨‡§∞: {', '.join(extracted_info['case_numbers'][:2])}")
            info_found = True
        
        # Phone numbers
        if extracted_info.get('phones'):
            summary_parts.append(f"‚Ä¢ ‡§´‡•ã‡§® ‡§®‡§Ç‡§¨‡§∞: {', '.join(extracted_info['phones'][:2])}")
            info_found = True
        
        # Email addresses
        if extracted_info.get('emails'):
            summary_parts.append(f"‚Ä¢ ‡§à‡§Æ‡•á‡§≤: {', '.join(extracted_info['emails'][:2])}")
            info_found = True
        
        # Addresses
        if extracted_info.get('addresses'):
            summary_parts.append(f"‚Ä¢ ‡§™‡§§‡§æ: {extracted_info['addresses'][0][:100]}...")
            info_found = True
        
        # If it looks like a resume/CV
        if extracted_info.get('is_resume'):
            summary_parts.append("")
            summary_parts.append("üéì ‡§Ø‡§π ‡§è‡§ï ‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á/CV ‡§≤‡§ó ‡§∞‡§π‡§æ ‡§π‡•à:")
            if extracted_info.get('skills'):
                summary_parts.append(f"‚Ä¢ ‡§ï‡•å‡§∂‡§≤: {', '.join(extracted_info['skills'][:5])}")
            if extracted_info.get('education'):
                summary_parts.append(f"‚Ä¢ ‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ: {', '.join(extracted_info['education'][:2])}")
            if extracted_info.get('experience'):
                summary_parts.append(f"‚Ä¢ ‡§Ö‡§®‡•Å‡§≠‡§µ: {', '.join(extracted_info['experience'][:2])}")
            info_found = True
        
        if not info_found:
            summary_parts.append("‚Ä¢ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä")
        
        # Add first 500 characters of actual text as preview
        summary_parts.append("")
        summary_parts.append("üìÑ ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂:")
        clean_text = ' '.join(text.split())[:500]
        summary_parts.append(f"'{clean_text}...'")
        
        # Add recommendations based on document type
        summary_parts.append("")
        summary_parts.append("‚úÖ ‡§∏‡•Å‡§ù‡§æ‡§µ:")
        
        recommendations = {
            'legal_notice': [
                "‚Ä¢ ‡§á‡§∏ ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§ï‡§æ ‡§ú‡§µ‡§æ‡§¨ 15-30 ‡§¶‡§ø‡§® ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç",
                "‚Ä¢ ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§∏‡§≤‡§æ‡§π ‡§≤‡•á‡§Ç",
                "‚Ä¢ NALSA ‡§π‡•á‡§≤‡•ç‡§™‡§≤‡§æ‡§á‡§®: 15100"
            ],
            'fir': [
                "‚Ä¢ FIR ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§Æ‡§ø‡§≤‡•á‡§Ç",
                "‚Ä¢ ‡§ß‡§æ‡§∞‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≤‡•á‡§Ç"
            ],
            'court_order': [
                "‚Ä¢ ‡§Ü‡§¶‡•á‡§∂ ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡•á‡§Ç",
                "‚Ä¢ ‡§Ö‡§ó‡§≤‡•Ä ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§Ø‡§æ‡§¶ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡•á‡§Ç"
            ],
            'land_record': [
                "‚Ä¢ ‡§§‡§π‡§∏‡•Ä‡§≤ ‡§∏‡•á ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç",
                "‚Ä¢ ‡§Æ‡•Ç‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§™‡§ü‡§µ‡§æ‡§∞‡•Ä ‡§∏‡•á ‡§Æ‡§ø‡§≤‡•á‡§Ç"
            ],
            'government_letter': [
                "‚Ä¢ ‡§∏‡§Æ‡§Ø ‡§∏‡•Ä‡§Æ‡§æ ‡§ï‡§æ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç",
                "‚Ä¢ ‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§Ç"
            ],
            'agreement': [
                "‚Ä¢ ‡§∏‡§≠‡•Ä ‡§∂‡§∞‡•ç‡§§‡•á‡§Ç ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∏‡•á ‡§™‡§¢‡§º‡•á‡§Ç",
                "‚Ä¢ ‡§è‡§ï ‡§ï‡•â‡§™‡•Ä ‡§Ö‡§™‡§®‡•á ‡§™‡§æ‡§∏ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§ú‡§æ‡§Å‡§ö ‡§ï‡§∞‡§µ‡§æ‡§è‡§Ç"
            ],
            'resume': [
                "‚Ä¢ ‡§Ö‡§™‡§®‡•Ä ‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ ‡§∏‡§π‡•Ä ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§ ‡§ï‡§∞‡•á‡§Ç",
                "‚Ä¢ ‡§∏‡§≠‡•Ä ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§π‡•Ä ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è"
            ],
            'general': [
                "‚Ä¢ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç",
                "‚Ä¢ ‡§Ø‡§¶‡§ø ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§π‡•à ‡§§‡•ã ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡•á‡§Ç",
                "‚Ä¢ NALSA ‡§π‡•á‡§≤‡•ç‡§™‡§≤‡§æ‡§á‡§®: 15100"
            ]
        }
        
        for rec in recommendations.get(doc_type, recommendations['general']):
            summary_parts.append(rec)
        
        return '\n'.join(summary_parts)
    
    def _extract_all_info(self, text):
        """Extract all possible information from text"""
        import re
        
        info = {
            'names': [],
            'dates': [],
            'amounts': [],
            'case_numbers': [],
            'phones': [],
            'emails': [],
            'addresses': [],
            'skills': [],
            'education': [],
            'experience': [],
            'is_resume': False
        }
        
        # Detect if it's a resume - require multiple resume-related keywords
        resume_keywords = ['resume', 'cv', 'curriculum vitae', 'objective', 'experience', 
                          'education', 'skills', 'qualifications', 'career',
                          '‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á', '‡§Ö‡§®‡•Å‡§≠‡§µ', '‡§∂‡§ø‡§ï‡•ç‡§∑‡§æ', '‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ']
        text_lower = text.lower()
        resume_keyword_count = sum(1 for kw in resume_keywords if kw in text_lower)
        # Require at least 3 resume keywords for detection
        if resume_keyword_count >= 3:
            info['is_resume'] = True
        
        # Extract dates with better validation
        date_patterns = [
            r'\b(?:0?[1-9]|[12][0-9]|3[01])[/\-](?:0?[1-9]|1[0-2])[/\-]\d{2,4}\b',  # DD/MM/YYYY with validation
            r'\b\d{1,2}\s+(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{2,4}\b',
            r'\b\d{1,2}\s+(?:‡§ú‡§®‡§µ‡§∞‡•Ä|‡§´‡§∞‡§µ‡§∞‡•Ä|‡§Æ‡§æ‡§∞‡•ç‡§ö|‡§Ö‡§™‡•ç‡§∞‡•à‡§≤|‡§Æ‡§à|‡§ú‡•Ç‡§®|‡§ú‡•Å‡§≤‡§æ‡§à|‡§Ö‡§ó‡§∏‡•ç‡§§|‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞|‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞|‡§®‡§µ‡§Ç‡§¨‡§∞|‡§¶‡§ø‡§∏‡§Ç‡§¨‡§∞)\s+\d{2,4}\b',
        ]
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if match not in info['dates'] and len(info['dates']) < 5:
                    info['dates'].append(match)
        
        # Extract amounts
        amount_patterns = [
            r'(?:rs\.?|‚Çπ|inr)\s*([0-9,]+(?:\.[0-9]+)?)',
            r'([0-9,]+(?:\.[0-9]+)?)\s*(?:rupees|‡§∞‡•Å‡§™‡§Ø‡•á)',
        ]
        for pattern in amount_patterns:
            info['amounts'].extend(re.findall(pattern, text, re.IGNORECASE))
        
        # Extract phone numbers
        phone_pattern = r'\b(?:\+91[\-\s]?)?[6-9]\d{9}\b'
        info['phones'] = re.findall(phone_pattern, text)
        
        # Extract email addresses
        email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        info['emails'] = re.findall(email_pattern, text)
        
        # Extract case/reference numbers
        case_patterns = [
            r'(?:case\s*no|ref\s*no|file\s*no|‡§ï‡•á‡§∏\s*‡§®‡§Ç)[.:]*\s*([A-Z0-9/\-]+)',
            r'(?:fir\s*no|‡§è‡§´‡§Ü‡§à‡§Ü‡§∞)[.:]*\s*([0-9/\-]+)',
        ]
        for pattern in case_patterns:
            info['case_numbers'].extend(re.findall(pattern, text, re.IGNORECASE))
        
        # Extract names (capitalized words that look like names)
        # Look for patterns like "Name: John Doe" or common name indicators
        name_patterns = [
            r'(?:name|‡§®‡§æ‡§Æ|applicant|complainant|petitioner|respondent)[:\s]+([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,2})',
            r'(?:mr\.|ms\.|mrs\.|shri|smt\.?|‡§∂‡•ç‡§∞‡•Ä|‡§∂‡•ç‡§∞‡•Ä‡§Æ‡§§‡•Ä)\s+([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,2})',
        ]
        for pattern in name_patterns:
            info['names'].extend(re.findall(pattern, text, re.IGNORECASE))
        
        # For resumes - extract skills
        if info['is_resume']:
            skill_section = re.search(r'(?:skills|technical\s*skills|‡§ï‡•å‡§∂‡§≤)[:\s]*([^\n]+(?:\n[^\n]+){0,5})', text, re.IGNORECASE)
            if skill_section:
                skills_text = skill_section.group(1)
                # Split by common delimiters
                skills = re.split(r'[,;‚Ä¢\n|]', skills_text)
                info['skills'] = [s.strip() for s in skills if s.strip() and len(s.strip()) < 30][:10]
            
            # Extract education
            edu_pattern = r'(?:b\.?tech|m\.?tech|b\.?e|m\.?e|b\.?sc|m\.?sc|b\.?a|m\.?a|b\.?com|m\.?com|mba|bba|ph\.?d|12th|10th|graduation|post.?graduation)'
            info['education'] = list(set(re.findall(edu_pattern, text, re.IGNORECASE)))[:5]
            
            # Extract years of experience
            exp_pattern = r'(\d+\+?\s*(?:years?|yrs?)(?:\s+of)?\s+(?:experience|exp))'
            info['experience'] = re.findall(exp_pattern, text, re.IGNORECASE)[:3]
        
        # Extract addresses (look for PIN codes and surrounding text)
        address_pattern = r'[A-Za-z0-9,.\s\-]+(?:pin|‡§™‡§ø‡§®)?[:\s]*\d{6}'
        info['addresses'] = re.findall(address_pattern, text, re.IGNORECASE)[:2]
        
        return info

    def _extract_key_points(self, text, doc_type):
        """Extract key points from the document"""
        key_points = []
        
        # Look for common patterns
        patterns = {
            'case_number': r'(?:case\s*no|‡§ï‡•á‡§∏\s*‡§®‡§Ç|‡§™‡•ç‡§∞‡§ï‡§∞‡§£\s*‡§ï‡•ç‡§∞‡§Æ‡§æ‡§Ç‡§ï)[.:]*\s*([A-Z0-9/\-]+)',
            'section': r'(?:‡§ß‡§æ‡§∞‡§æ|section|u/s)\s*([0-9]+(?:\s*[,/]\s*[0-9]+)*)',
            'amount': r'(?:‡§∞‡•Å|rs|‚Çπ|rupees)[.:]*\s*([0-9,]+)',
            'plot': r'(?:‡§ñ‡§∏‡§∞‡§æ|plot|‡§™‡•ç‡§≤‡•â‡§ü|‡§ñ‡§æ‡§§‡§æ)\s*(?:‡§®‡§Ç|no|‡§®‡§Ç‡§¨‡§∞)?[.:]*\s*([0-9/\-]+)',
            'fir_no': r'(?:fir\s*no|‡§è‡§´‡§Ü‡§à‡§Ü‡§∞)[.:]*\s*([0-9/\-]+)',
        }
        
        for pattern_name, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                if pattern_name == 'case_number':
                    key_points.append(f"‡§ï‡•á‡§∏ ‡§®‡§Ç‡§¨‡§∞: {matches[0]}")
                elif pattern_name == 'section':
                    key_points.append(f"‡§ß‡§æ‡§∞‡§æ: {matches[0]}")
                elif pattern_name == 'amount':
                    key_points.append(f"‡§∞‡§æ‡§∂‡§ø: ‚Çπ{matches[0]}")
                elif pattern_name == 'plot':
                    key_points.append(f"‡§ñ‡§∏‡§∞‡§æ/‡§™‡•ç‡§≤‡•â‡§ü ‡§®‡§Ç‡§¨‡§∞: {matches[0]}")
                elif pattern_name == 'fir_no':
                    key_points.append(f"FIR ‡§®‡§Ç‡§¨‡§∞: {matches[0]}")
        
        # Add type-specific points
        type_points = {
            'legal_notice': ['‡§Ø‡§π ‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§π‡•à', '‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ ‡§ú‡§º‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•à'],
            'fir': ['‡§Ø‡§π ‡§™‡•Å‡§≤‡§ø‡§∏ FIR ‡§π‡•à', '‡§ï‡§æ‡§®‡•Ç‡§®‡•Ä ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§µ‡§æ‡§π‡•Ä ‡§∂‡•Å‡§∞‡•Ç'],
            'court_order': ['‡§Ø‡§π ‡§®‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§≤‡§Ø ‡§ï‡§æ ‡§Ü‡§¶‡•á‡§∂ ‡§π‡•à', '‡§™‡§æ‡§≤‡§® ‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§π‡•à'],
            'land_record': ['‡§Ø‡§π ‡§≠‡•Ç‡§Æ‡§ø ‡§∞‡§ø‡§ï‡•â‡§∞‡•ç‡§° ‡§π‡•à', '‡§Æ‡§æ‡§≤‡§ø‡§ï‡§æ‡§®‡§æ ‡§π‡§ï‡§º ‡§ï‡§æ ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£'],
            'government_letter': ['‡§Ø‡§π ‡§∏‡§∞‡§ï‡§æ‡§∞‡•Ä ‡§™‡§§‡•ç‡§∞ ‡§π‡•à', '‡§∏‡§Æ‡§Ø ‡§∏‡•Ä‡§Æ‡§æ ‡§ï‡§æ ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∞‡§ñ‡•á‡§Ç'],
            'agreement': ['‡§Ø‡§π ‡§∏‡§Æ‡§ù‡•å‡§§‡§æ ‡§™‡§§‡•ç‡§∞ ‡§π‡•à', '‡§∂‡§∞‡•ç‡§§‡•á‡§Ç ‡§¨‡§æ‡§ß‡•ç‡§Ø‡§ï‡§æ‡§∞‡•Ä ‡§π‡•à‡§Ç'],
            'resume': ['‡§Ø‡§π ‡§è‡§ï ‡§∞‡§ø‡§ú‡§º‡•ç‡§Ø‡•Ç‡§Æ‡•á/CV ‡§π‡•à', '‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ ‡§î‡§∞ ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç'],
        }
        
        key_points.extend(type_points.get(doc_type, ['‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º', '‡§∏‡§≤‡§æ‡§π ‡§≤‡•á‡§Ç']))
        
        return key_points[:6]  # Max 6 points

    def _extract_dates(self, text):
        """Extract important dates from text with validation"""
        dates = []
        
        # Common date patterns - with proper day/month validation
        date_patterns = [
            # DD/MM/YYYY or DD-MM-YYYY (validates day 1-31, month 1-12)
            r'\b(?:0?[1-9]|[12][0-9]|3[01])[/-](?:0?[1-9]|1[0-2])[/-](?:\d{4}|\d{2})\b',
            # Named months: 1-31 MonthName YYYY
            r'\b(?:0?[1-9]|[12][0-9]|3[01])\s+(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{2,4}\b',
            # Named months in Hindi
            r'\b(?:0?[1-9]|[12][0-9]|3[01])\s+(?:‡§ú‡§®‡§µ‡§∞‡•Ä|‡§´‡§∞‡§µ‡§∞‡•Ä|‡§Æ‡§æ‡§∞‡•ç‡§ö|‡§Ö‡§™‡•ç‡§∞‡•à‡§≤|‡§Æ‡§à|‡§ú‡•Ç‡§®|‡§ú‡•Å‡§≤‡§æ‡§à|‡§Ö‡§ó‡§∏‡•ç‡§§|‡§∏‡§ø‡§§‡§Ç‡§¨‡§∞|‡§Ö‡§ï‡•ç‡§ü‡•Ç‡§¨‡§∞|‡§®‡§µ‡§Ç‡§¨‡§∞|‡§¶‡§ø‡§∏‡§Ç‡§¨‡§∞)\s+\d{2,4}\b',
        ]
        
        found_dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # Avoid duplicates
                if match not in found_dates and len(found_dates) < 5:
                    found_dates.append(match)
        
        # Format dates
        for date in found_dates:
            dates.append({
                'date': date,
                'description': '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§Æ‡•á‡§Ç ‡§â‡§≤‡•ç‡§≤‡•á‡§ñ‡§ø‡§§ ‡§§‡§ø‡§•‡§ø'
            })
        
        return dates

    def _get_recommended_actions(self, doc_type):
        """Get recommended actions based on document type"""
        actions = {
            'legal_notice': [
                '‡§ò‡§¨‡§∞‡§æ‡§è‡§Ç ‡§®‡§π‡•Ä‡§Ç - ‡§Ø‡§π ‡§∏‡§ø‡§∞‡•ç‡§´ ‡§®‡•ã‡§ü‡§ø‡§∏ ‡§π‡•à, ‡§ï‡•ã‡§∞‡•ç‡§ü ‡§ï‡•á‡§∏ ‡§®‡§π‡•Ä‡§Ç',
                '‡§ú‡§ø‡§≤‡§æ ‡§µ‡§ø‡§ß‡§ø‡§ï ‡§∏‡•á‡§µ‡§æ ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§∏‡•á ‡§Æ‡•Å‡§´‡•ç‡§§ ‡§∏‡§≤‡§æ‡§π ‡§≤‡•á‡§Ç',
                '15-30 ‡§¶‡§ø‡§® ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§Ç',
                '‡§∏‡§≠‡•Ä ‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§æ‡§ó‡§ú‡§º‡§æ‡§§ ‡§á‡§ï‡§ü‡•ç‡§†‡§æ ‡§ï‡§∞‡•á‡§Ç',
                'NALSA ‡§π‡•á‡§≤‡•ç‡§™‡§≤‡§æ‡§á‡§® 15100 ‡§™‡§∞ ‡§ï‡•â‡§≤ ‡§ï‡§∞‡•á‡§Ç'
            ],
            'fir': [
                'FIR ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§Ö‡§™‡§®‡•á ‡§™‡§æ‡§∏ ‡§∞‡§ñ‡•á‡§Ç',
                '‡§§‡•Å‡§∞‡§Ç‡§§ ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§Æ‡§ø‡§≤‡•á‡§Ç',
                '‡§Ö‡§ó‡•ç‡§∞‡§ø‡§Æ ‡§ú‡§Æ‡§æ‡§®‡§§ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡•á‡§Ç',
                '‡§ó‡§µ‡§æ‡§π‡•ã‡§Ç ‡§ï‡•Ä ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§á‡§ï‡§ü‡•ç‡§†‡§æ ‡§ï‡§∞‡•á‡§Ç',
                'NALSA ‡§π‡•á‡§≤‡•ç‡§™‡§≤‡§æ‡§á‡§® 15100 ‡§™‡§∞ ‡§ï‡•â‡§≤ ‡§ï‡§∞‡•á‡§Ç'
            ],
            'court_order': [
                '‡§Ü‡§¶‡•á‡§∂ ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç',
                '‡§Ö‡§ó‡§≤‡•Ä ‡§§‡§æ‡§∞‡•Ä‡§ñ ‡§®‡•ã‡§ü ‡§ï‡§∞‡•á‡§Ç',
                '‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§§‡•Å‡§∞‡§Ç‡§§ ‡§Æ‡§ø‡§≤‡•á‡§Ç',
                '‡§Ü‡§¶‡•á‡§∂ ‡§ï‡§æ ‡§∏‡§Æ‡§Ø ‡§™‡§∞ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡•á‡§Ç',
                '‡§Ö‡§™‡•Ä‡§≤ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§Ç'
            ],
            'land_record': [
                '‡§§‡§π‡§∏‡•Ä‡§≤ ‡§∏‡•á ‡§§‡§æ‡§ú‡§º‡§æ ‡§ñ‡§§‡•å‡§®‡•Ä ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç',
                '‡§®‡§æ‡§Æ ‡§î‡§∞ ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡§´‡§≤ ‡§ú‡§æ‡§Å‡§ö‡•á‡§Ç',
                '‡§ï‡•ã‡§à ‡§µ‡§ø‡§µ‡§æ‡§¶ ‡§π‡•ã ‡§§‡•ã SDM ‡§ï‡•ã ‡§≤‡§ø‡§ñ‡•á‡§Ç',
                '‡§Æ‡•Ç‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç',
                '‡§ú‡§º‡§∞‡•Ç‡§∞‡§§ ‡§π‡•ã ‡§§‡•ã ‡§∞‡§æ‡§ú‡§∏‡•ç‡§µ ‡§µ‡§ø‡§≠‡§æ‡§ó ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç'
            ],
            'government_letter': [
                '‡§∏‡§Æ‡§Ø ‡§∏‡•Ä‡§Æ‡§æ ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡•á‡§Ç',
                '‡§ú‡§µ‡§æ‡§¨ ‡§≤‡§ø‡§ñ‡§ø‡§§ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§Ç',
                '‡§™‡§§‡•ç‡§∞ ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§∞‡§ñ‡•á‡§Ç',
                '‡§∏‡§Ç‡§¨‡§Ç‡§ß‡§ø‡§§ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§æ‡§≤‡§Ø ‡§∏‡•á ‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ï‡§∞‡•á‡§Ç',
                'RTI ‡§∏‡•á ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§Æ‡§æ‡§Å‡§ó ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç'
            ],
            'agreement': [
                '‡§∏‡§≠‡•Ä ‡§∂‡§∞‡•ç‡§§‡•á‡§Ç ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∏‡•á ‡§™‡§¢‡§º‡•á‡§Ç',
                '‡§ú‡•ã ‡§∏‡§Æ‡§ù ‡§® ‡§Ü‡§è, ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§ï‡§∞‡§µ‡§æ‡§è‡§Ç',
                '‡§ú‡§¨‡§∞‡§¶‡§∏‡•ç‡§§‡•Ä ‡§Æ‡•á‡§Ç ‡§∏‡§æ‡§á‡§® ‡§® ‡§ï‡§∞‡•á‡§Ç',
                '‡§ó‡§µ‡§æ‡§π‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§æ‡§Æ‡§®‡•á ‡§π‡§∏‡•ç‡§§‡§æ‡§ï‡•ç‡§∑‡§∞ ‡§ï‡§∞‡•á‡§Ç',
                '‡§è‡§ï ‡§ï‡•â‡§™‡•Ä ‡§Ö‡§™‡§®‡•á ‡§™‡§æ‡§∏ ‡§∞‡§ñ‡•á‡§Ç'
            ],
            'resume': [
                '‡§Ö‡§™‡§®‡•Ä ‡§Ø‡•ã‡§ó‡•ç‡§Ø‡§§‡§æ ‡§∏‡§π‡•Ä ‡§§‡§∞‡•Ä‡§ï‡•á ‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡•ç‡§§‡•Å‡§§ ‡§ï‡§∞‡•á‡§Ç',
                '‡§∏‡§≠‡•Ä ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º‡•ã‡§Ç ‡§ï‡•Ä ‡§ï‡•â‡§™‡•Ä ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§∞‡§ñ‡•á‡§Ç',
                '‡§∏‡§Ç‡§™‡§∞‡•ç‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§∏‡§π‡•Ä ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è',
                'LinkedIn ‡§Ø‡§æ ‡§ú‡•â‡§¨ ‡§™‡•ã‡§∞‡•ç‡§ü‡§≤ ‡§™‡§∞ ‡§Ö‡§™‡§°‡•á‡§ü ‡§∞‡§ñ‡•á‡§Ç',
                '‡§®‡§ø‡§Ø‡§Æ‡§ø‡§§ ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§Ö‡§™‡§°‡•á‡§ü ‡§ï‡§∞‡§§‡•á ‡§∞‡§π‡•á‡§Ç'
            ]
        }
        
        default_actions = [
            '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§ï‡•ã ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§∏‡•á ‡§™‡§¢‡§º‡•á‡§Ç',
            '‡§ú‡§ø‡§≤‡§æ ‡§µ‡§ø‡§ß‡§ø‡§ï ‡§∏‡•á‡§µ‡§æ ‡§™‡•ç‡§∞‡§æ‡§ß‡§ø‡§ï‡§∞‡§£ ‡§∏‡•á ‡§Æ‡•Å‡§´‡•ç‡§§ ‡§∏‡§≤‡§æ‡§π ‡§≤‡•á‡§Ç',
            '‡§Æ‡•Ç‡§≤ ‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú‡§º ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§∞‡§ñ‡•á‡§Ç',
            'NALSA ‡§π‡•á‡§≤‡•ç‡§™‡§≤‡§æ‡§á‡§® 15100 ‡§™‡§∞ ‡§ï‡•â‡§≤ ‡§ï‡§∞‡•á‡§Ç',
            '‡§µ‡§ï‡•Ä‡§≤ ‡§∏‡•á ‡§™‡§∞‡§æ‡§Æ‡§∞‡•ç‡§∂ ‡§≤‡•á‡§Ç'
        ]
        
        return actions.get(doc_type, default_actions)
